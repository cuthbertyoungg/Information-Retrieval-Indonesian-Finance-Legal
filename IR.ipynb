{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14144252,"sourceType":"datasetVersion","datasetId":9014109},{"sourceId":14148190,"sourceType":"datasetVersion","datasetId":9016893},{"sourceId":14151777,"sourceType":"datasetVersion","datasetId":9019649},{"sourceId":14173969,"sourceType":"datasetVersion","datasetId":9034857}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:56:24.635381Z","iopub.execute_input":"2025-12-14T04:56:24.635948Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-5.2.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\nDownloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-5.2.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:56:45.906094Z","iopub.execute_input":"2025-12-14T04:56:45.906842Z","iopub.status.idle":"2025-12-14T04:56:50.717405Z","shell.execute_reply.started":"2025-12-14T04:56:45.906818Z","shell.execute_reply":"2025-12-14T04:56:50.716665Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip -q install pdfplumber jiwer pandas\n\nimport re\nfrom pathlib import Path\nimport pandas as pd\nimport pdfplumber\nfrom jiwer import wer, cer\n\n# ------------------ CONFIG ------------------\nPDF_ROOT = \"/kaggle/input/peraturan-keuangan/UU Keuangan\"\nGT_CSV   = \"/kaggle/input/gt-wer-cer/ground truth WER CER - Sheet2.csv\"\nOUT_CSV  = \"wer_cer_final_fixed_heading.csv\"\nSHOW_EXAMPLES = 8\n\n# ------------------ PDF -> TEXT ------------------\ndef extract_text_from_pdf(pdf_path):\n    out = []\n    try:\n        with pdfplumber.open(pdf_path) as pdf:\n            for p in pdf.pages:\n                out.append(p.extract_text() or \"\")\n    except Exception as e:\n        print(f\"[WARN] gagal baca {pdf_path}: {e}\")\n        return \"\"\n    return \"\\n\".join(out)\n\n# ------------------ NORMALIZATION ------------------\ndef strong_normalize(text):\n    if not text:\n        return \"\"\n    text = text.replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"-\\n\", \"\", text)                 # join hyphen linebreak\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n\n    # robust PASAL\n    text = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", text, flags=re.I)\n\n    # remove url/email/web noise\n    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text, flags=re.I)\n    text = re.sub(r\"\\b\\w+@\\w+\\.\\w+\\b\", \" \", text)\n    text = re.sub(r\"\\bweb\\b|\\bwww\\b\", \" \", text, flags=re.I)\n\n    # remove page markers\n    text = re.sub(r\"^\\s*(halaman|page)\\s*\\d+\\s*$\", \" \", text, flags=re.I|re.M)\n    text = re.sub(r\"^\\s*\\d+\\s*$\", \" \", text, flags=re.M)\n\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n    return text.strip()\n\n# ------------------ BATANG TUBUH ONLY (fallback safe) ------------------\ndef keep_only_batang_tubuh(text):\n    full = text\n    lower = text.lower()\n    m = re.search(r\"\\bpenjelasan\\b\", lower)\n    if not m:\n        return full\n\n    cut = full[:m.start()].strip()\n    # fallback kalau potongan terlalu pendek / tidak ada pasal sama sekali\n    if len(cut) < 500:\n        return full\n    if not re.search(r\"\\bpasal\\s+\\d+\\b\", cut, flags=re.I):\n        return full\n    return cut\n\n# ------------------ PASAL CANONICAL ------------------\ndef canonical_pasal(p):\n    p = str(p).strip()\n    p = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", p, flags=re.I)\n    p = re.sub(r\"\\s+\", \" \", p).strip()\n    m = re.match(r\"(?i)pasal\\s+(\\d+)([a-z]?)\", p)\n    if not m:\n        return p\n    return f\"Pasal {m.group(1)}{m.group(2).upper()}\"\n\n# ------------------ HEADING DETECTION (KEY FIX) ------------------\ndef is_pasal_heading(line: str):\n    \"\"\"\n    True hanya jika baris itu benar-benar judul pasal.\n    Mencegah false split karena kalimat: \"sebagaimana dimaksud dalam Pasal 3 ...\"\n    Heuristik:\n    - 'Pasal <angka><opsional huruf>' muncul di awal baris\n    - baris sangat pendek (misal: \"Pasal 7\" atau \"Pasal 7.\")\n    - tidak mengandung kata-kata kalimat (sebagaimana/dimaksud/ayat/huruf/dalam)\n    \"\"\"\n    s = line.strip()\n    if not s:\n        return False\n\n    # harus start dengan Pasal\n    m = re.match(r\"(?i)^pasal\\s+\\d+[a-z]?\\b\", s)\n    if not m:\n        return False\n\n    # buang ekor punctuation untuk pengecekan \"pendek\"\n    s_clean = re.sub(r\"[.:;,\\-–—()\\[\\]]+\", \" \", s)\n    s_clean = re.sub(r\"\\s+\", \" \", s_clean).strip()\n\n    # token harus sedikit (heading biasanya 2 token: Pasal + angka)\n    toks = s_clean.split()\n    if len(toks) > 3:\n        return False\n\n    # hindari baris yang jelas kalimat/referensi\n    bad_markers = [\"sebagaimana\", \"dimaksud\", \"dalam\", \"ayat\", \"huruf\", \"angka\", \"pada\"]\n    low = s.lower()\n    if any(b in low for b in bad_markers):\n        return False\n\n    return True\n\n# ------------------ QUALITY SCORING (avoid \"cukup jelas\") ------------------\ndef pasal_quality_score(text):\n    t = (text or \"\").lower()\n    score = 0\n    if re.search(r\"\\(\\s*1\\s*\\)\", t):   # ada ayat (1)\n        score += 5\n    if \"cukup jelas\" in t:\n        score -= 5\n    score += min(len(t) // 400, 5)     # tambah skor jika panjang (wajar untuk pasal normatif)\n    return score\n\n# ------------------ EXTRACT PASAL (STATE MACHINE, fixed heading) ------------------\ndef extract_pasal(text):\n    text = strong_normalize(text)\n    text = keep_only_batang_tubuh(text)\n\n    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n\n    pasal_map = {}\n    current = None\n    buffer = []\n\n    for line in lines:\n        if is_pasal_heading(line):\n            # simpan pasal sebelumnya\n            if current and buffer:\n                cand = \" \".join(buffer).strip()\n                if (current not in pasal_map) or (pasal_quality_score(cand) > pasal_quality_score(pasal_map[current])):\n                    pasal_map[current] = cand\n\n            current = canonical_pasal(line)\n            buffer = [line]\n        else:\n            if current:\n                buffer.append(line)\n\n    # simpan terakhir\n    if current and buffer:\n        cand = \" \".join(buffer).strip()\n        if (current not in pasal_map) or (pasal_quality_score(cand) > pasal_quality_score(pasal_map[current])):\n            pasal_map[current] = cand\n\n    return pasal_map\n\n# ------------------ METRIC NORMALIZATION ------------------\ndef normalize_metric(t):\n    t = (t or \"\").lower().replace(\"\\n\", \" \")\n    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t, flags=re.I)\n    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\n# ============================================================\n# RUN\n# ============================================================\ntry:\n    from IPython.display import display\nexcept:\n    def display(x):\n        print(x)\n\nprint(\"=== [1] Load Ground Truth ===\")\ngt = pd.read_csv(GT_CSV)\ngt_use = gt[[\"doc_id\", \"pasal\", \"reference_text\"]].copy()\ngt_use[\"doc_id\"] = gt_use[\"doc_id\"].astype(str).str.strip()\ngt_use[\"pasal\"]  = gt_use[\"pasal\"].apply(canonical_pasal)\n\ntarget_docs = set(gt_use[\"doc_id\"])\nprint(\"GT rows:\", len(gt_use))\nprint(\"Unique doc_id:\", len(target_docs))\n\nprint(\"\\n=== [2] Index PDFs ===\")\npdf_index = {p.name: str(p) for p in Path(PDF_ROOT).rglob(\"*.pdf\")}\nprint(\"Total PDFs found:\", len(pdf_index))\n\nprint(\"\\n=== [3] Extract Pasal only for target docs ===\")\nextracted = {}\nmissing = []\nfor doc in sorted(target_docs):\n    path = pdf_index.get(doc)\n    if not path:\n        missing.append(doc)\n        continue\n    raw = extract_text_from_pdf(path)\n    if not raw.strip():\n        continue\n    pasal_map = extract_pasal(raw)\n    for k, v in pasal_map.items():\n        extracted[(doc, k)] = v\n\nprint(\"Extracted (doc,pasal) pairs:\", len(extracted))\nprint(\"Missing PDFs:\", len(missing))\nif missing:\n    print(\"Missing examples:\", missing[:10])\n\nprint(\"\\n=== [4] Evaluate WER/CER ===\")\nrows = []\nskipped = 0\nfor _, r in gt_use.iterrows():\n    key = (r[\"doc_id\"], r[\"pasal\"])\n    hyp = extracted.get(key, \"\")\n    if not hyp.strip():\n        skipped += 1\n        continue\n\n    ref_n = normalize_metric(r[\"reference_text\"])\n    hyp_n = normalize_metric(hyp)\n\n    rows.append({\n        \"doc_id\": r[\"doc_id\"],\n        \"pasal\": r[\"pasal\"],\n        \"WER\": wer(ref_n, hyp_n),\n        \"CER\": cer(ref_n, hyp_n),\n        \"ref_words\": len(ref_n.split()),\n        \"hyp_words\": len(hyp_n.split())\n    })\n\ndf = pd.DataFrame(rows)\ndf.to_csv(OUT_CSV, index=False)\n\nmean_WER = df[\"WER\"].mean() if len(df) else None\nmean_CER = df[\"CER\"].mean() if len(df) else None\nmedian_WER = df[\"WER\"].median() if len(df) else None\nmedian_CER = df[\"CER\"].median() if len(df) else None\n\nprint(\"\\n=== SUMMARY ===\")\nprint(\"Evaluated pairs:\", len(df))\nprint(\"Skipped (no match):\", skipped)\nprint(\"Mean WER:\", mean_WER)\nprint(\"Median WER:\", median_WER)\nprint(\"Mean CER:\", mean_CER)\nprint(\"Median CER:\", median_CER)\nprint(\"Saved:\", OUT_CSV)\n\nprint(\"\\n=== [5] Examples (GT vs EXTRACTED) ===\")\nshown = 0\nfor _, r in gt_use.iterrows():\n    key = (r[\"doc_id\"], r[\"pasal\"])\n    if key not in extracted:\n        continue\n    print(f\"\\n--- {key[0]} | {key[1]} ---\")\n    print(\"[GT]  :\", r[\"reference_text\"][:350])\n    print(\"[EXT] :\", extracted[key][:900])\n    shown += 1\n    if shown >= SHOW_EXAMPLES:\n        break\n\nprint(\"\\n=== [6] Worst WER top 10 ===\")\nif len(df):\n    display(df.sort_values(\"WER\", ascending=False).head(10))\nelse:\n    print(\"No evaluated rows. Cek doc_id/pasal match.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T04:56:55.403798Z","iopub.execute_input":"2025-12-14T04:56:55.404562Z","iopub.status.idle":"2025-12-14T04:58:24.496288Z","shell.execute_reply.started":"2025-12-14T04:56:55.404525Z","shell.execute_reply":"2025-12-14T04:58:24.495710Z"}},"outputs":[{"name":"stdout","text":"=== [1] Load Ground Truth ===\nGT rows: 50\nUnique doc_id: 47\n\n=== [2] Index PDFs ===\nTotal PDFs found: 3721\n\n=== [3] Extract Pasal only for target docs ===\nExtracted (doc,pasal) pairs: 941\nMissing PDFs: 0\n\n=== [4] Evaluate WER/CER ===\n\n=== SUMMARY ===\nEvaluated pairs: 37\nSkipped (no match): 13\nMean WER: 0.17328717666075727\nMedian WER: 0.05102040816326531\nMean CER: 0.13276682411795904\nMedian CER: 0.02046783625730994\nSaved: wer_cer_final_fixed_heading.csv\n\n=== [5] Examples (GT vs EXTRACTED) ===\n\n--- PP_Nomor_2_Tahun_2012_cde4.pdf | Pasal 12 ---\n[GT]  : (1) Menteri/pimpinan lembaga pemerintah non kementerian\ndapat mengusulkan besaran hibah dan daftar nama\nPemerintah Daerah yang diusulkan sebagai penerima hibah\nkepada Menteri berdasarkan penetapan Pemerintah untuk\nhibah kepada Pemerintah Daerah yang bersumber dari\npenerimaan dalam negeri.\n(2) Menteri/pimpinan lembaga pemerintah non kementerian\nmeng\n[EXT] : Pasal 12 (1) Menteri/pimpinan lembaga pemerintah non kementerian dapat mengusulkan besaran hibah dan daftar nama Pemerintah Daerah yang diusulkan sebagai penerima hibah kepada Menteri berdasarkan penetapan Pemerintah untuk hibah kepada Pemerintah Daerah yang bersumber dari penerimaan dalam negeri. (2) Menteri/pimpinan lembaga pemerintah non kementerian mengusulkan besaran hibah dan daftar nama Pemerintah Daerah yang diusulkan sebagai penerima hibah kepada Menteri berdasarkan penetapan Menteri atas alokasi peruntukkan pinjaman luar negeri dan hibah luar negeri. (3) Pengusulan Pemerintah Daerah sebagai penerima hibah sebagaimana dimaksud pada ayat (1) dan ayat (2) dengan mempertimbangkan: a. kapasitas fiskal daerah; b. Daerah yang ditentukan oleh Pemberi Hibah Luar Negeri; c. Daerah yang memenuhi persyaratan yang ditentukan oleh kementerian negara/lembaga pemerintah non kementerian terkait\n\n--- PP_Nomor_2_Tahun_2012.pdf | Pasal 6 ---\n[GT]  : (1) Hibah kepada Pemerintah Daerah sebagaimana dimaksud\ndalam Pasal 2 huruf a merupakan salah satu sumber\npenerimaan Daerah untuk mendanai penyelenggaraan\nurusan yang menjadi kewenangan Pemerintah Daerah\ndalam kerangka hubungan keuangan antara Pemerintah\ndan Pemerintah Daerah.\n(2) Hibah kepada Pemerintah Daerah sebagaimana dimaksud\npada ayat (1) da\n[EXT] : Pasal 6 (1) Hibah kepada Pemerintah Daerah sebagaimana dimaksud dalam Pasal 2 huruf a merupakan salah satu sumber penerimaan Daerah untuk mendanai penyelenggaraan urusan yang menjadi kewenangan Pemerintah Daerah dalam kerangka hubungan keuangan antara Pemerintah dan Pemerintah Daerah. (2) Hibah kepada Pemerintah Daerah sebagaimana dimaksud pada ayat (1) dapat diteruskan kepada badan usaha milik daerah. (3) Hibah kepada Pemerintah Daerah sebagaimana dimaksud pada ayat (1) diprioritaskan untuk penyelenggaraan Pelayanan Publik sesuai dengan ketentuan peraturan perundang-undangan. (4) Hibah kepada Pemerintah Daerah sebagaimana dimaksud pada ayat (1) dilaksanakan dengan memperhatikan stabilitas dan keseimbangan fiskal.\n\n--- PP_Nomor_24_Tahun_2018_1dec.pdf | Pasal 4 ---\n[GT]  : Peraturan Pemerintah ini mengatur mengenai:\na. jenis, pemohon, dan penerbit Perizinan Berusaha;\nb. pelaksanaanPerizinanBerusaha;\nc. reformasi Perizinan Berusaha sektor;\nd. sistem OSS;\ne. tembaga OSS;\nf. pendanaan OSS;\ng. insentif atau disinsentif pelaksanaan Perizinan Berusaha\nmelalui OSS;\nh. penyelesaian permasalahan dan hambatan Perizina\n[EXT] : Pasal 4 Peraturan Pemerintah ini mengatur mengenai: a. jenis, pemohon, dan penerbit Perizinan Berusaha; b. pelaksanaanPerizinanBerusaha; c. reformasi Perizinan Berusaha sektor; d. sistem OSS; e. tembaga OSS; f. pendanaan OSS; g. insentif atau disinsentif pelaksanaan Perizinan Berusaha melalui OSS; h. penyelesaian permasalahan dan hambatan Perizinan Berusaha melalui OSS; dan i. sanksi. BAB II JENIS, PEMOHON, DAN PENERBIT PERIZINAN BERUSAHA Bagian Kesatu Jenis Perizinan Berusaha\n\n--- PP_Nomor_24_Tahun_2018_936e.pdf | Pasal 6 ---\n[GT]  : Pemohon Perizinan Berusaha terdiri atas:\na. Pelaku Usaha perseorangan; dan\nb. Pelaku Usaha non perseorangan.\nPelaku Usaha perseorangan sebagaimana dimaksud pada\nayat (1) huruf a merupakan orang perorangan penduduk\nIndonesia yang cakap untuk bertindak dan melakukan\nperbuatan hukum.\nPelaku Usaha non perseorangan sebagaimana dimaksud\npada ayat (l) hur\n[EXT] : Pasal 6 (l) Pemohon Perizinan Berusaha terdiri atas: a. Pelaku Usaha perseorangan; dan b. Pelaku Usaha non perseorangan. (2\\ Pelaku Usaha perseorangan sebagaimana dimaksud pada ayat (1) huruf a merupakan orang perorangan penduduk Indonesia yang cakap untuk bertindak dan melakukan perbuatan hukum. (3) Pelaku Usaha non perseorangan sebagaimana dimaksud pada ayat (l) huruf b terdiri atas: a. perseroan terbatas; b. perusahaan umum; c. perusahaan umum daerah; d. badan hukum lainnya yang dimiliki oleh negara; e. badan layanan umum; f. lembaga penyiaran; C. badan usaha yang didirikan oleh yayasan; h. koperasi; i. persekutuan komanditer (commanditaire uennootschap); j. persekutuan lirma (uenootschap onderfirmal; dan k. persekutuanperdata.\n\n--- PP_Nomor_24_Tahun_2018_c71d.pdf | Pasal 6 ---\n[GT]  : (1) Presiden Republik Indonesia memegang kekuasaan\npemerintahan sesuai dengan Undang-Undang Dasar\nNegara Republik Indonesia Tahun 1945.\n(2) Kekuasaan pemerintahan sebagaimana dimaksud pada\nayat (1) diuraikan dalam berbagai urusan pemerintahan\nyang pelaksanaannya dilakukan oleh kementerian negara\ndan penyelenggara Pemerintahan Daerah.\n(3) Urusan pem\n[EXT] : Pasal 6 (l) Pemohon Perizinan Berusaha terdiri atas: a. Pelaku Usaha perseorangan; dan b. Pelaku Usaha non perseorangan. (2\\ Pelaku Usaha perseorangan sebagaimana dimaksud pada ayat (1) huruf a merupakan orang perorangan penduduk Indonesia yang cakap untuk bertindak dan melakukan perbuatan hukum. (3) Pelaku Usaha non perseorangan sebagaimana dimaksud pada ayat (l) huruf b terdiri atas: a. perseroan terbatas; b. perusahaan umum; c. perusahaan umum daerah; d. badan hukum lainnya yang dimiliki oleh negara; e. badan layanan umum; f. lembaga penyiaran; C. badan usaha yang didirikan oleh yayasan; h. koperasi; i. persekutuan komanditer (commanditaire uennootschap); j. persekutuan lirma (uenootschap onderfirmal; dan k. persekutuanperdata.\n\n--- 1._Salinan_UU_Nomor_15_Tahun_2025_a051.pdf | Pasal 7 ---\n[GT]  : Laporan Arus Kas Tahun Anggaran 2024 sebagaimana\ndimaksud dalam Pasal 2 ayat (1) huruf e memberikan\ninformasi keuangan sebagai berikut:\na. jumlah Arus Kas Bersih dari Aktivitas Operasi sebesar\nminus Rp154.149.780.567.127,00 (seratus lima puluh\nempat triliun seratus empat puluh sembilan miliar tujuh\nratus delapan puluh juta lima ratus enam puluh tuj\n[EXT] : Pasal 7 Laporan Arus Kas Tahun Anggaran 2024 sebagaimana dimaksud dalam Pasal 2 ayat (1) huruf e memberikan informasi keuangan sebagai berikut: a. jumlah Arus Kas Bersih dari Aktivitas Operasi sebesar minus Rp154.L49.78O.567.L27,OO (seratus lima puluh empat triliun seratus empat puluh sembilan miliar tujuh ratus delapan puluh juta lima ratus enam puluh tujuh ribu seratus dua puluh tqluh rupiah); b. jumlah Arus Kas Bersih dari Aktivitas Investasi sebesar minus Rp414.4O3.152.94L.181,00 (empat ratus empat belas triliun empat ratus tiga miliar seratus lima puluh dua juta sembilan ratus empat puluh satu ribu seratus delapan puluh satu rupiah); c. jumlah Arus Kas Bersih dari Aktivitas Pendanaan sebesar Rp614.28O.048.971.953,O0 (enam ratus empat belas triliun dua ratus delapan puluh miliar empat puluh delapan juta sembilan ratus tujuh puluh satu ribu sembilan ratus lima puluh tiga rupiah); dan \n\n--- 1._Salinan_UU_Nomor_15_Tahun_2025_cce2.pdf | Pasal 5 ---\n[GT]  : Neraca per 31 Desember 2024 sebagaimana dimaksud dalam\nPasal 2 ayat (1) hunrf c memberikan informasi keuangan\nsebagai berikut:\na. jumlah Aset sebesar Rp13.692.365.851.510.229,00 (tiga\nbelas kuadriliun enam ratus sembilan puluh dua triliun\ntiga ratus enam puluh lima miliar delapan ratus lima\npuluh satu juta lima ratus sepuluh ribu dua ratus du\n[EXT] : Pasal 5 Neraca per 31 Desember 2024 sebagaimana dimaksud dalam Pasal 2 ayat (1) hunrf c memberikan informasi keuangan sebagai berikut: a. jumlah Aset sebesar Rp13.692.365.851.510.229,00 (tiga belas kuadriliun enam ratus sembilan puluh dua triliun tiga ratus enam puluh lima miliar delapan ratus lima puluh satu juta lima ratus sepuluh ribu dua ratus dua puluh sembilan rupiah); b.jumlah... SK No254269A PRESIDEN REPUBLIK ]NDONES]A -7 b. jumlah Kewajiban sebesar Rp 1O.269.O 18 .258.24L.877,OO (sepuluh kuadriliun dua ratus enam puluh sembilan triliun delapan belas miliar dua ratus lima puluh delapan juta dua ratus empat puluh satu ribu delapan ratus tujuh puluh tqjuh rupiah); dan c. jumlah Ekuitas sebesar Rp3.423.3a7.593.268.352,OO (tiga kuadriliun empat ratus dua puluh tiga triliun tiga ratus empat puluh tujuh miliar lima ratus sembilan puluh tiga juta dua ratus enam puluh delapan ribu tiga r\n\n--- 1._Salinan_UU_Nomor_15_Tahun_2025_b3d9.pdf | Pasal 5 ---\n[GT]  : Neraca per 31 Desember 2024 sebagaimana dimaksud dalam\nPasal 2 ayat (1) huruf c memberikan informasi keuangan\nsebagai berikut:a. jumlah Aset sebesar Rp13.692.365.851.510.229,00 (tiga\nbelas kuadriliun enam ratus sembilan puluh dua triliun\ntiga ratus enam puluh lima miliar delapan ratus lima\npuluh satu juta lima ratus sepuluh ribu dua ratus dua\np\n[EXT] : Pasal 5 Neraca per 31 Desember 2024 sebagaimana dimaksud dalam Pasal 2 ayat (1) hunrf c memberikan informasi keuangan sebagai berikut: a. jumlah Aset sebesar Rp13.692.365.851.510.229,00 (tiga belas kuadriliun enam ratus sembilan puluh dua triliun tiga ratus enam puluh lima miliar delapan ratus lima puluh satu juta lima ratus sepuluh ribu dua ratus dua puluh sembilan rupiah); b.jumlah... SK No254269A PRESIDEN REPUBLIK ]NDONES]A -7 b. jumlah Kewajiban sebesar Rp 1O.269.O 18 .258.24L.877,OO (sepuluh kuadriliun dua ratus enam puluh sembilan triliun delapan belas miliar dua ratus lima puluh delapan juta dua ratus empat puluh satu ribu delapan ratus tujuh puluh tqjuh rupiah); dan c. jumlah Ekuitas sebesar Rp3.423.3a7.593.268.352,OO (tiga kuadriliun empat ratus dua puluh tiga triliun tiga ratus empat puluh tujuh miliar lima ratus sembilan puluh tiga juta dua ratus enam puluh delapan ribu tiga r\n\n=== [6] Worst WER top 10 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                               doc_id     pasal       WER  \\\n4                     PP_Nomor_24_Tahun_2018_c71d.pdf   Pasal 6  1.516667   \n5          1._Salinan_UU_Nomor_15_Tahun_2025_a051.pdf   Pasal 7  1.338710   \n31                         PP Nomor 34 Tahun 2023.pdf   Pasal 3  0.848837   \n36  PERBUP MBD NOMOR 33 TAHUN 2022 TARIF AIR MINUM...   Pasal 3  0.510204   \n2                     PP_Nomor_24_Tahun_2018_1dec.pdf   Pasal 4  0.306122   \n20  Peraturan_Menteri_Keuangan_(PMK)_Nomor_21PMK.0...  Pasal 13  0.191489   \n28                                 perpu 001 1959.pdf   Pasal 3  0.175000   \n19  Perubahan_Kedua_atas_Peraturan_Presiden_Nomor_...  Pasal 10  0.164179   \n7          1._Salinan_UU_Nomor_15_Tahun_2025_b3d9.pdf   Pasal 5  0.136986   \n6          1._Salinan_UU_Nomor_15_Tahun_2025_cce2.pdf   Pasal 5  0.123288   \n\n         CER  ref_words  hyp_words  \n4   0.942740         60        103  \n5   1.316876        186        426  \n31  0.793722         86         19  \n36  0.370821         49         74  \n2   0.279330         49         64  \n20  0.137834         94        112  \n28  0.196507         40         47  \n19  0.128623         67         78  \n7   0.083825        146        158  \n6   0.080283        146        158  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>pasal</th>\n      <th>WER</th>\n      <th>CER</th>\n      <th>ref_words</th>\n      <th>hyp_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>PP_Nomor_24_Tahun_2018_c71d.pdf</td>\n      <td>Pasal 6</td>\n      <td>1.516667</td>\n      <td>0.942740</td>\n      <td>60</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1._Salinan_UU_Nomor_15_Tahun_2025_a051.pdf</td>\n      <td>Pasal 7</td>\n      <td>1.338710</td>\n      <td>1.316876</td>\n      <td>186</td>\n      <td>426</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>PP Nomor 34 Tahun 2023.pdf</td>\n      <td>Pasal 3</td>\n      <td>0.848837</td>\n      <td>0.793722</td>\n      <td>86</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>PERBUP MBD NOMOR 33 TAHUN 2022 TARIF AIR MINUM...</td>\n      <td>Pasal 3</td>\n      <td>0.510204</td>\n      <td>0.370821</td>\n      <td>49</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PP_Nomor_24_Tahun_2018_1dec.pdf</td>\n      <td>Pasal 4</td>\n      <td>0.306122</td>\n      <td>0.279330</td>\n      <td>49</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Peraturan_Menteri_Keuangan_(PMK)_Nomor_21PMK.0...</td>\n      <td>Pasal 13</td>\n      <td>0.191489</td>\n      <td>0.137834</td>\n      <td>94</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>perpu 001 1959.pdf</td>\n      <td>Pasal 3</td>\n      <td>0.175000</td>\n      <td>0.196507</td>\n      <td>40</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Perubahan_Kedua_atas_Peraturan_Presiden_Nomor_...</td>\n      <td>Pasal 10</td>\n      <td>0.164179</td>\n      <td>0.128623</td>\n      <td>67</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1._Salinan_UU_Nomor_15_Tahun_2025_b3d9.pdf</td>\n      <td>Pasal 5</td>\n      <td>0.136986</td>\n      <td>0.083825</td>\n      <td>146</td>\n      <td>158</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1._Salinan_UU_Nomor_15_Tahun_2025_cce2.pdf</td>\n      <td>Pasal 5</td>\n      <td>0.123288</td>\n      <td>0.080283</td>\n      <td>146</td>\n      <td>158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# 1-RUN: EXTRACT ALL PDF -> PASAL CHUNKS (FOR IR INDEXING)\n# Using SAME logic:\n# - strong_normalize\n# - keep_only_batang_tubuh (fallback aman)\n# - is_pasal_heading (anti \"Pasal X\" in-sentence)\n# - state-machine + quality scoring\n#\n# Output:\n# - pasal_chunks.jsonl  (recommended for IR)\n# - pasal_chunks.csv    (optional, same content)\n# - stats_per_doc.csv\n# ============================================================\n\n# !pip -q install pdfplumber pandas\n\nimport re, json\nfrom pathlib import Path\nimport pandas as pd\nimport pdfplumber\n\n# ------------------ CONFIG ------------------\nPDF_ROOT = \"/kaggle/input/peraturan-keuangan/UU Keuangan\"   # <-- root folder (bisa banyak subfolder)\nOUT_JSONL = \"pasal_chunks.jsonl\"\nOUT_CSV   = \"pasal_chunks.csv\"\nOUT_STATS = \"stats_per_doc.csv\"\n\n# Safety limits (biar gak kebablasan kalau PDF rusak)\nMAX_CHARS_PER_PASAL = 20000\nMIN_WORDS_TO_KEEP   = 10\n\n# ------------------ PDF -> TEXT ------------------\ndef extract_text_from_pdf(pdf_path):\n    out = []\n    try:\n        with pdfplumber.open(pdf_path) as pdf:\n            for p in pdf.pages:\n                out.append(p.extract_text() or \"\")\n    except Exception as e:\n        print(f\"[WARN] gagal baca {pdf_path}: {e}\")\n        return \"\"\n    return \"\\n\".join(out)\n\n# ------------------ NORMALIZATION ------------------\ndef strong_normalize(text):\n    if not text:\n        return \"\"\n    text = text.replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"-\\n\", \"\", text)\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n\n    text = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", text, flags=re.I)\n\n    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text, flags=re.I)\n    text = re.sub(r\"\\b\\w+@\\w+\\.\\w+\\b\", \" \", text)\n    text = re.sub(r\"\\bweb\\b|\\bwww\\b\", \" \", text, flags=re.I)\n\n    text = re.sub(r\"^\\s*(halaman|page)\\s*\\d+\\s*$\", \" \", text, flags=re.I|re.M)\n    text = re.sub(r\"^\\s*\\d+\\s*$\", \" \", text, flags=re.M)\n\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n    return text.strip()\n\ndef keep_only_batang_tubuh(text):\n    full = text\n    lower = text.lower()\n    m = re.search(r\"\\bpenjelasan\\b\", lower)\n    if not m:\n        return full\n\n    cut = full[:m.start()].strip()\n    if len(cut) < 500:\n        return full\n    if not re.search(r\"\\bpasal\\s+\\d+\\b\", cut, flags=re.I):\n        return full\n    return cut\n\n# ------------------ PASAL CANONICAL ------------------\ndef canonical_pasal(p):\n    p = str(p).strip()\n    p = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", p, flags=re.I)\n    p = re.sub(r\"\\s+\", \" \", p).strip()\n    m = re.match(r\"(?i)pasal\\s+(\\d+)([a-z]?)\", p)\n    if not m:\n        return p\n    return f\"Pasal {m.group(1)}{m.group(2).upper()}\"\n\n# ------------------ HEADING DETECTION ------------------\ndef is_pasal_heading(line: str):\n    s = line.strip()\n    if not s:\n        return False\n    m = re.match(r\"(?i)^pasal\\s+\\d+[a-z]?\\b\", s)\n    if not m:\n        return False\n\n    s_clean = re.sub(r\"[.:;,\\-–—()\\[\\]]+\", \" \", s)\n    s_clean = re.sub(r\"\\s+\", \" \", s_clean).strip()\n    toks = s_clean.split()\n    if len(toks) > 3:\n        return False\n\n    bad_markers = [\"sebagaimana\", \"dimaksud\", \"dalam\", \"ayat\", \"huruf\", \"angka\", \"pada\"]\n    low = s.lower()\n    if any(b in low for b in bad_markers):\n        return False\n    return True\n\n# ------------------ QUALITY SCORING ------------------\ndef pasal_quality_score(text):\n    t = (text or \"\").lower()\n    score = 0\n    if re.search(r\"\\(\\s*1\\s*\\)\", t):\n        score += 5\n    if \"cukup jelas\" in t:\n        score -= 5\n    score += min(len(t) // 400, 5)\n    return score\n\n# ------------------ EXTRACT PASAL ------------------\ndef extract_pasal_map(raw_text):\n    text = strong_normalize(raw_text)\n    text = keep_only_batang_tubuh(text)\n\n    if not text:\n        return {}\n\n    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n    pasal_map = {}\n    current = None\n    buffer = []\n\n    for line in lines:\n        if is_pasal_heading(line):\n            if current and buffer:\n                cand = \" \".join(buffer).strip()\n                if len(cand) > MAX_CHARS_PER_PASAL:\n                    cand = cand[:MAX_CHARS_PER_PASAL]\n                if (current not in pasal_map) or (pasal_quality_score(cand) > pasal_quality_score(pasal_map[current])):\n                    pasal_map[current] = cand\n\n            current = canonical_pasal(line)\n            buffer = [line]\n        else:\n            if current:\n                buffer.append(line)\n\n    if current and buffer:\n        cand = \" \".join(buffer).strip()\n        if len(cand) > MAX_CHARS_PER_PASAL:\n            cand = cand[:MAX_CHARS_PER_PASAL]\n        if (current not in pasal_map) or (pasal_quality_score(cand) > pasal_quality_score(pasal_map[current])):\n            pasal_map[current] = cand\n\n    return pasal_map\n\n# ============================================================\n# RUN: PROCESS ALL PDF\n# ============================================================\npdf_paths = list(Path(PDF_ROOT).rglob(\"*.pdf\"))\nprint(\"Total PDFs found:\", len(pdf_paths))\n\nall_rows = []\nstats = []\n\nfor i, p in enumerate(pdf_paths, start=1):\n    doc_id = p.name\n    pdf_path = str(p)\n\n    raw = extract_text_from_pdf(pdf_path)\n    if not raw.strip():\n        stats.append({\"doc_id\": doc_id, \"path\": pdf_path, \"num_pasal\": 0, \"status\": \"empty_or_unreadable\"})\n        continue\n\n    pasal_map = extract_pasal_map(raw)\n\n    kept = 0\n    for pasal, chunk_text in pasal_map.items():\n        n_words = len(chunk_text.split())\n        if n_words < MIN_WORDS_TO_KEEP:\n            continue\n\n        all_rows.append({\n            \"doc_id\": doc_id,\n            \"path\": pdf_path,\n            \"pasal\": pasal,\n            \"chunk_text\": chunk_text,\n            \"n_words\": n_words\n        })\n        kept += 1\n\n    stats.append({\"doc_id\": doc_id, \"path\": pdf_path, \"num_pasal\": kept, \"status\": \"ok\" if kept > 0 else \"no_pasal_detected\"})\n\n    if i % 25 == 0:\n        print(f\"[{i}/{len(pdf_paths)}] processed... total_chunks={len(all_rows)}\")\n\n# ============================================================\n# SAVE OUTPUTS\n# ============================================================\ndf_chunks = pd.DataFrame(all_rows)\ndf_stats  = pd.DataFrame(stats)\n\n# JSONL (recommended)\nwith open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n    for row in all_rows:\n        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\n# CSV (optional)\ndf_chunks.to_csv(OUT_CSV, index=False)\ndf_stats.to_csv(OUT_STATS, index=False)\n\nprint(\"\\nDONE ✅\")\nprint(\"Total pasal chunks:\", len(df_chunks))\nprint(\"Saved:\", OUT_JSONL, OUT_CSV, OUT_STATS)\n\nprint(\"\\nTop 10 docs by extracted pasal:\")\nprint(df_stats.sort_values(\"num_pasal\", ascending=False).head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:05:00.275318Z","iopub.execute_input":"2025-12-14T05:05:00.275616Z","iopub.status.idle":"2025-12-14T08:26:33.280132Z","shell.execute_reply.started":"2025-12-14T05:05:00.275594Z","shell.execute_reply":"2025-12-14T08:26:33.279326Z"}},"outputs":[{"name":"stdout","text":"Total PDFs found: 3721\n[25/3721] processed... total_chunks=217\n[50/3721] processed... total_chunks=734\n[75/3721] processed... total_chunks=1123\n[100/3721] processed... total_chunks=1474\n[125/3721] processed... total_chunks=1919\n[150/3721] processed... total_chunks=2223\n[175/3721] processed... total_chunks=2385\n[200/3721] processed... total_chunks=2574\n[225/3721] processed... total_chunks=2766\n[250/3721] processed... total_chunks=2993\n[275/3721] processed... total_chunks=3268\n[300/3721] processed... total_chunks=3571\n[325/3721] processed... total_chunks=3882\n[350/3721] processed... total_chunks=4051\n[375/3721] processed... total_chunks=4302\n[400/3721] processed... total_chunks=4539\n[425/3721] processed... total_chunks=4829\n","output_type":"stream"},{"name":"stderr","text":"Cannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\nCannot set gray non-stroke color because /'P13' is an invalid float value\nCannot set gray non-stroke color because /'P14' is an invalid float value\nCannot set gray non-stroke color because /'P15' is an invalid float value\nCannot set gray non-stroke color because /'P16' is an invalid float value\nCannot set gray non-stroke color because /'P17' is an invalid float value\nCannot set gray non-stroke color because /'P18' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P19' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P20' is an invalid float value\nCannot set gray non-stroke color because /'P21' is an invalid float value\nCannot set gray non-stroke color because /'P22' is an invalid float value\nCannot set gray non-stroke color because /'P22' is an invalid float value\n","output_type":"stream"},{"name":"stdout","text":"[450/3721] processed... total_chunks=5083\n[475/3721] processed... total_chunks=5480\n[500/3721] processed... total_chunks=5791\n[525/3721] processed... total_chunks=6294\n[WARN] gagal baca /kaggle/input/peraturan-keuangan/UU Keuangan/PERDA NOMOR 9 TAHUN 2012 TENTANG PENCABUTAN PERDA NOMOR 19 TAHUN 2003 TENTANG RETRIBUSI TERHADAP HASIL PRODUKSI BAHAN OLA~1.pdf: No /Root object! - Is this really a PDF?\n[550/3721] processed... total_chunks=6440\n[575/3721] processed... total_chunks=6626\n[600/3721] processed... total_chunks=6986\n[625/3721] processed... total_chunks=7215\n[650/3721] processed... total_chunks=7712\n[675/3721] processed... total_chunks=8033\n[700/3721] processed... total_chunks=8318\n[725/3721] processed... total_chunks=8593\n[750/3721] processed... total_chunks=8685\n[775/3721] processed... total_chunks=8828\n[800/3721] processed... total_chunks=9189\n[825/3721] processed... total_chunks=9437\n[850/3721] processed... total_chunks=9813\n[875/3721] processed... total_chunks=10020\n[900/3721] processed... total_chunks=10278\n[925/3721] processed... total_chunks=10426\n[950/3721] processed... total_chunks=10582\n[975/3721] processed... total_chunks=10949\n[1000/3721] processed... total_chunks=11228\n[1025/3721] processed... total_chunks=11492\n[1050/3721] processed... total_chunks=11782\n[1075/3721] processed... total_chunks=12083\n[1100/3721] processed... total_chunks=12346\n[1125/3721] processed... total_chunks=12532\n[1150/3721] processed... total_chunks=12724\n[1175/3721] processed... total_chunks=12989\n[1200/3721] processed... total_chunks=13150\n[1225/3721] processed... total_chunks=13328\n[1250/3721] processed... total_chunks=13510\n[1275/3721] processed... total_chunks=13745\n[1300/3721] processed... total_chunks=13899\n","output_type":"stream"},{"name":"stderr","text":"Cannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\nCannot set gray non-stroke color because /'P13' is an invalid float value\nCannot set gray non-stroke color because /'P14' is an invalid float value\nCannot set gray non-stroke color because /'P15' is an invalid float value\nCannot set gray non-stroke color because /'P16' is an invalid float value\nCannot set gray non-stroke color because /'P17' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\nCannot set gray non-stroke color because /'P13' is an invalid float value\nCannot set gray non-stroke color because /'P14' is an invalid float value\nCannot set gray non-stroke color because /'P15' is an invalid float value\n","output_type":"stream"},{"name":"stdout","text":"[1325/3721] processed... total_chunks=14190\n[1350/3721] processed... total_chunks=14357\n[1375/3721] processed... total_chunks=14516\n[1400/3721] processed... total_chunks=14727\n[1425/3721] processed... total_chunks=15058\n[1450/3721] processed... total_chunks=15336\n[1475/3721] processed... total_chunks=15579\n[1500/3721] processed... total_chunks=15845\n[1525/3721] processed... total_chunks=16142\n[1550/3721] processed... total_chunks=16426\n[1575/3721] processed... total_chunks=16705\n[1600/3721] processed... total_chunks=17054\n[1625/3721] processed... total_chunks=17269\n[1650/3721] processed... total_chunks=17496\n[1675/3721] processed... total_chunks=17746\n[1700/3721] processed... total_chunks=18035\n[1725/3721] processed... total_chunks=18380\n[1750/3721] processed... total_chunks=18525\n[1775/3721] processed... total_chunks=18782\n[1800/3721] processed... total_chunks=19281\n[1825/3721] processed... total_chunks=19559\n[1850/3721] processed... total_chunks=19721\n[1875/3721] processed... total_chunks=20124\n[1900/3721] processed... total_chunks=20404\n[1925/3721] processed... total_chunks=20710\n[1950/3721] processed... total_chunks=20941\n","output_type":"stream"},{"name":"stderr","text":"Cannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\n","output_type":"stream"},{"name":"stdout","text":"[1975/3721] processed... total_chunks=21119\n[2000/3721] processed... total_chunks=21599\n[2025/3721] processed... total_chunks=21850\n[2050/3721] processed... total_chunks=22198\n[2075/3721] processed... total_chunks=22628\n[2100/3721] processed... total_chunks=23009\n[2125/3721] processed... total_chunks=23276\n[2150/3721] processed... total_chunks=23494\n[2175/3721] processed... total_chunks=23766\n[2200/3721] processed... total_chunks=23951\n[2225/3721] processed... total_chunks=24188\n[2250/3721] processed... total_chunks=24365\n[2275/3721] processed... total_chunks=24812\n[2300/3721] processed... total_chunks=25255\n[2325/3721] processed... total_chunks=25567\n[2350/3721] processed... total_chunks=25811\n[2375/3721] processed... total_chunks=26060\n[2400/3721] processed... total_chunks=26541\n[2425/3721] processed... total_chunks=26799\n[2450/3721] processed... total_chunks=26997\n[2475/3721] processed... total_chunks=27210\n[2500/3721] processed... total_chunks=27700\n[2525/3721] processed... total_chunks=27948\n[2550/3721] processed... total_chunks=28272\n[2575/3721] processed... total_chunks=28555\n[2600/3721] processed... total_chunks=29035\n[2625/3721] processed... total_chunks=29302\n[2650/3721] processed... total_chunks=29522\n","output_type":"stream"},{"name":"stderr","text":"Cannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P2' is an invalid float value\nCannot set gray non-stroke color because /'P3' is an invalid float value\nCannot set gray non-stroke color because /'P4' is an invalid float value\nCannot set gray non-stroke color because /'P5' is an invalid float value\nCannot set gray non-stroke color because /'P6' is an invalid float value\nCannot set gray non-stroke color because /'P7' is an invalid float value\nCannot set gray non-stroke color because /'P8' is an invalid float value\nCannot set gray non-stroke color because /'P9' is an invalid float value\nCannot set gray non-stroke color because /'P10' is an invalid float value\nCannot set gray non-stroke color because /'P11' is an invalid float value\nCannot set gray non-stroke color because /'P12' is an invalid float value\nCannot set gray non-stroke color because /'P13' is an invalid float value\nCannot set gray non-stroke color because /'P14' is an invalid float value\nCannot set gray non-stroke color because /'P15' is an invalid float value\nCannot set gray non-stroke color because /'P16' is an invalid float value\nCannot set gray non-stroke color because /'P17' is an invalid float value\n","output_type":"stream"},{"name":"stdout","text":"[2675/3721] processed... total_chunks=29727\n[2700/3721] processed... total_chunks=29910\n[2725/3721] processed... total_chunks=30147\n[WARN] gagal baca /kaggle/input/peraturan-keuangan/UU Keuangan/Permenkes Nomor 19 Tahun 2020.pdf: No /Root object! - Is this really a PDF?\n[2750/3721] processed... total_chunks=30359\n[2775/3721] processed... total_chunks=30556\n[2800/3721] processed... total_chunks=30887\n[2825/3721] processed... total_chunks=31062\n[2850/3721] processed... total_chunks=31480\n[2875/3721] processed... total_chunks=31694\n[2900/3721] processed... total_chunks=31899\n[2925/3721] processed... total_chunks=32318\n[2950/3721] processed... total_chunks=32628\n[2975/3721] processed... total_chunks=32865\n[3000/3721] processed... total_chunks=33031\n[3025/3721] processed... total_chunks=33412\n[3050/3721] processed... total_chunks=33660\n[3075/3721] processed... total_chunks=33948\n[3100/3721] processed... total_chunks=34133\n[3125/3721] processed... total_chunks=34415\n[WARN] gagal baca /kaggle/input/peraturan-keuangan/UU Keuangan/Peraturan_Menteri_Kesehatan_Nomor_7_Tahun_2020.pdf: No /Root object! - Is this really a PDF?\n[3150/3721] processed... total_chunks=34654\n[3175/3721] processed... total_chunks=34953\n[3200/3721] processed... total_chunks=35167\n[3225/3721] processed... total_chunks=35428\n[3250/3721] processed... total_chunks=35595\n[3275/3721] processed... total_chunks=35852\n[3300/3721] processed... total_chunks=36059\n[3325/3721] processed... total_chunks=36433\n[3350/3721] processed... total_chunks=36949\n[3375/3721] processed... total_chunks=37207\n[3400/3721] processed... total_chunks=37380\n[3425/3721] processed... total_chunks=37602\n[3450/3721] processed... total_chunks=37857\n[3475/3721] processed... total_chunks=37987\n[3500/3721] processed... total_chunks=38345\n[3525/3721] processed... total_chunks=38665\n","output_type":"stream"},{"name":"stderr","text":"Cannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\nCannot set gray non-stroke color because /'P0' is an invalid float value\nCannot set gray non-stroke color because /'P1' is an invalid float value\n","output_type":"stream"},{"name":"stdout","text":"[3550/3721] processed... total_chunks=38914\n[3575/3721] processed... total_chunks=39077\n[3600/3721] processed... total_chunks=39381\n[3625/3721] processed... total_chunks=39710\n[3650/3721] processed... total_chunks=39862\n[3675/3721] processed... total_chunks=40077\n[3700/3721] processed... total_chunks=40374\n\nDONE ✅\nTotal pasal chunks: 40724\nSaved: pasal_chunks.jsonl pasal_chunks.csv stats_per_doc.csv\n\nTop 10 docs by extracted pasal:\n                                                 doc_id  \\\n2591  Peraturan_Badan_Pengawasan_Keuangan_dan_Pemban...   \n1922                     UU_Nomor_1_Tahun_2022_28d2.pdf   \n633                      UU_Nomor_1_Tahun_2022_38de.pdf   \n1987                     UU_Nomor_1_Tahun_2022_511d.pdf   \n3018                     UU_Nomor_1_Tahun_2022_80a4.pdf   \n2272                     UU_Nomor_1_Tahun_2022_c86f.pdf   \n2915                     UU_Nomor_1_Tahun_2022_2cfb.pdf   \n2490                          UU Nomor 1 Tahun 2022.pdf   \n2063                     UU_Nomor_1_Tahun_2022_eb05.pdf   \n1594                          UU_Nomor_1_Tahun_2022.pdf   \n\n                                                   path  num_pasal status  \n2591  /kaggle/input/peraturan-keuangan/UU Keuangan/P...        184     ok  \n1922  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n633   /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n1987  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n3018  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n2272  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n2915  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n2490  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n2063  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n1594  /kaggle/input/peraturan-keuangan/UU Keuangan/U...        181     ok  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Sebelum Revisi GroundTruth","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# BAGIAN 1: INSTALL, IMPORT, CONFIG\n# =============================================================================\n# Install library yang diperlukan\n!pip -q install pandas numpy scikit-learn rank-bm25 sentence-transformers sacrebleu tqdm\n\nimport os, re, math, json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nimport sacrebleu\n\n# Cek GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device yang digunakan: {device}\")\n\n# ==== KONFIGURASI ====\nTOP_K_RERANK_CANDIDATES = 25  \nTOP_K_FETCH = 100           \nMAX_K_EVAL = 30             \nK_LEVELS = [5, 15, 30]       \n\n# ==== PATHS (SESUAIKAN JIKA PERLU) ====\nPASAL_CHUNKS_CSV = \"/kaggle/input/dataset-clean-and-ground-truth/pasal_chunks.csv\"\nQUERIES_CSV      = \"/kaggle/input/dataset-clean-and-ground-truth/ground truth uas  - queries.csv\"\nQRELS_CSV        = \"/kaggle/input/dataset-clean-and-ground-truth/ground truth uas  - qrels.csv\"\n\nOUT_DIR = \"ir_final_project\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# =============================================================================\n# BAGIAN 2: LOAD DATA & NORMALISASI\n# =============================================================================\nprint(\"Loading Data...\")\ncorpus_df  = pd.read_csv(PASAL_CHUNKS_CSV)\nqueries_df = pd.read_csv(QUERIES_CSV)\nqrels_df   = pd.read_csv(QRELS_CSV)\n\n# --- FUNGSI NORMALISASI ---\ndef smart_norm(text: str) -> str:\n    if not isinstance(text, str): text = str(text)\n    text = re.sub(r\"(?<=\\d)O(?=\\d)\", \"0\", text) # Fix OCR: '199O' -> '1990'\n    text = text.lower()\n    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text) # Hapus URL\n    text = re.sub(r\"[^a-z0-9\\.,\\(\\)\\s]\", \" \", text) # Pertahankan tanda baca penting hukum\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef canon_pasal(p):\n    p = str(p).strip()\n    p = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", p, flags=re.I)\n    p = re.sub(r\"\\s+\", \" \", p).strip()\n    m = re.match(r\"(?i)pasal\\s+(\\d+)([a-z]?)\", p)\n    if not m: return p\n    return f\"Pasal {m.group(1)}{m.group(2).upper()}\"\n\n# --- TERAPKAN NORMALISASI ---\ncorpus_df[\"pasal_canon\"] = corpus_df[\"pasal\"].apply(canon_pasal)\ncorpus_df[\"doc_fp\"] = corpus_df[\"doc_id\"].astype(str).str.strip()\ncorpus_df[\"chunk_text_norm\"] = corpus_df[\"chunk_text\"].apply(smart_norm)\n\nqueries_df[\"question_text_norm\"] = queries_df[\"question_text\"].apply(smart_norm)\n\nqrels_df[\"pasal_canon\"] = qrels_df[\"pasal\"].apply(canon_pasal)\nqrels_df[\"doc_fp\"] = qrels_df[\"doc_id\"].astype(str).str.strip()\nqrels_df[\"gold_passage_norm\"] = qrels_df[\"gold_passage\"].astype(str).apply(smart_norm)\n\n# List Cache untuk Akses Cepat\ncorpus_texts = corpus_df[\"chunk_text_norm\"].tolist()\ncorpus_docfp = corpus_df[\"doc_fp\"].tolist()\ncorpus_pasal = corpus_df[\"pasal_canon\"].tolist()\n\nprint(f\"Data Loaded. Corpus Size: {len(corpus_texts)}\")\n\n# =============================================================================\n# BAGIAN 3: ADVANCED GROUND TRUTH (ANTI FALSE-NEGATIVE)\n# =============================================================================\n# Kita butuh mapping konten untuk verifikasi duplikat\ndoc_id_to_text = {\n    str(row[\"doc_id\"]).strip(): row[\"chunk_text_norm\"] \n    for _, row in corpus_df.iterrows()\n}\n\nGLOBAL_QRELS_ENHANCED = {}\n\nfor qid, sub in qrels_df.groupby(\"question_id\"):\n    valid_pasals = set()\n    texts_by_pasal = {} \n    rel_map_fp = {}\n    gold_texts_list = [] # Untuk BLEU\n    \n    for _, r in sub.iterrows():\n        p_name = r[\"pasal_canon\"]\n        d_id = str(r[\"doc_id\"]).strip()\n        rel = int(r.get(\"relevance\", 1))\n        \n        rel_map_fp[d_id] = rel\n        \n        # Simpan teks emas untuk BLEU score\n        if isinstance(r[\"gold_passage_norm\"], str) and len(r[\"gold_passage_norm\"]) > 5:\n            gold_texts_list.append(r[\"gold_passage_norm\"])\n        \n        if isinstance(p_name, str):\n            valid_pasals.add(p_name)\n            if d_id in doc_id_to_text:\n                if p_name not in texts_by_pasal: texts_by_pasal[p_name] = []\n                texts_by_pasal[p_name].append(doc_id_to_text[d_id])\n            \n    GLOBAL_QRELS_ENHANCED[qid] = {\n        \"valid_pasals\": valid_pasals,  \n        \"rel_map_fp\": rel_map_fp,      \n        \"texts_by_pasal\": texts_by_pasal, \n        \"gold_texts\": list(set(gold_texts_list))\n    }\n\nprint(\"Ground Truth Enhanced Built.\")\n\n# =============================================================================\n# BAGIAN 4: HELPER FUNCTIONS (METRICS & LOGIC)\n# =============================================================================\n\ndef get_jaccard_sim(str1, str2):\n    \"\"\"Menghitung kemiripan kata antara dua teks\"\"\"\n    a = set(str(str1).split())\n    b = set(str(str2).split())\n    if len(a) == 0 or len(b) == 0: return 0.0\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef dynamic_hop_predictor(scores, model_type):\n    \"\"\"\n    LOGIKA \"JUJUR\" (Tanpa Oracle).\n    Menebak Single/Multi berdasarkan pola skor yang dihasilkan model.\n    \"\"\"\n    if len(scores) < 2: return \"single\"\n    \n    s1, s2 = scores[0], scores[1]\n    \n    # 1. Konversi Logit Reranker ke Probabilitas (0-1)\n    if \"RERANKER\" in model_type:\n        s1 = 1 / (1 + np.exp(-s1))\n        s2 = 1 / (1 + np.exp(-s2))\n    \n    if s1 <= 1e-9: return \"single\" # Safety check\n    \n    ratio = s2 / s1\n    \n    # 2. Threshold Ketat (Agar dominan Single Hop)\n    if model_type in [\"DENSE_E5\", \"HYBRID_RRF\"]:\n        # Dense skornya sangat rapat (0.8 vs 0.79). \n        # Multi Hop hanya jika ratio > 99.8% (Sangat identik)\n        if ratio > 0.998: return \"multi\"\n            \n    elif \"RERANKER\" in model_type:\n        # Reranker biasanya tegas. Jika probabilitasnya dekat (>98%), baru Multi.\n        if ratio > 0.98: return \"multi\"\n            \n    else: # BM25 / TFIDF\n        # Skor tidak terbatas. Jika rasionya > 98%, baru Multi.\n        if ratio > 0.98: return \"multi\"\n\n    return \"single\"\n\ndef is_prediction_correct(pred_fp, pred_pasal, pred_text, ground_truth):\n    \"\"\"\n    Menentukan Kebenaran Prediksi (Recall Hybrid).\n    Benar jika: (ID Cocok) ATAU (Nama Pasal Cocok DAN Isi Konten Mirip)\n    \"\"\"\n    # A. Cek ID Persis\n    if pred_fp in ground_truth[\"rel_map_fp\"]:\n        return True\n        \n    # B. Cek Nama Pasal + Isi Konten (Menangkap Duplikat/Chunking)\n    if pred_pasal in ground_truth[\"valid_pasals\"]:\n        valid_texts = ground_truth[\"texts_by_pasal\"].get(pred_pasal, [])\n        if not valid_texts: return False \n        \n        # Bandingkan dengan setiap variasi teks kunci jawaban\n        for val_text in valid_texts:\n            sim = get_jaccard_sim(pred_text, val_text)\n            if sim > 0.85: # Ambang batas 85% kemiripan\n                return True\n                \n    return False\n\ndef dedup_results(idx, scores, corpus_docfp, corpus_pasal, corpus_texts, k=30):\n    \"\"\"\n    Deduplikasi: Memastikan variasi pasal di output.\n    \"\"\"\n    seen_fp = set()\n    u_fps, u_pasals, u_texts, u_scores = [], [], [], []\n    \n    for i, s in zip(idx, scores):\n        fp = str(corpus_docfp[i])\n        if fp in seen_fp: continue\n        seen_fp.add(fp)\n        \n        u_fps.append(fp)\n        u_pasals.append(corpus_pasal[i])\n        u_texts.append(corpus_texts[i])\n        u_scores.append(s)\n        \n        if len(u_fps) >= k: break\n            \n    return u_fps, u_pasals, u_texts, np.array(u_scores)\n\ndef calculate_metrics_for_k(pred_fps, pred_pasals, pred_texts, top_scores, qid, model_name, k_val):\n    \"\"\"Menghitung metrik untuk nilai K tertentu\"\"\"\n    if qid not in GLOBAL_QRELS_ENHANCED: return None\n    gt = GLOBAL_QRELS_ENHANCED[qid]\n    \n    # Slice data sesuai K\n    curr_fps = pred_fps[:k_val]\n    curr_pasals = pred_pasals[:k_val]\n    curr_texts = pred_texts[:k_val]\n    \n    # 1. Hitung Hits (Benar/Salah) dengan Smart Check\n    hits = []\n    for i in range(len(curr_fps)):\n        is_hit = is_prediction_correct(curr_fps[i], curr_pasals[i], curr_texts[i], gt)\n        hits.append(1 if is_hit else 0)\n    \n    tp = sum(hits)\n    \n    # 2. Precision & Recall\n    precision = tp / k_val\n    # Recall = TP / Jumlah Pasal Unik yang Valid (Adil)\n    recall = tp / len(gt[\"valid_pasals\"]) if len(gt[\"valid_pasals\"]) > 0 else 0\n    \n    # 3. MRR\n    mrr = 0\n    for i, h in enumerate(hits):\n        if h: \n            mrr = 1/(i+1)\n            break\n            \n    # 4. NDCG\n    def dcg(r): return sum((2**v - 1)/math.log2(idx+2) for idx,v in enumerate(r))\n    ideal = [1] * len(gt[\"valid_pasals\"]) # Idealnya semua pasal valid ditemukan\n    ndcg = dcg(hits) / (dcg(ideal) + 1e-9)\n    \n    # 5. BLEU (Top-1)\n    best_bleu = 0.0\n    if gt[\"gold_texts\"] and len(curr_texts) > 0:\n        best_bleu = sacrebleu.sentence_bleu(curr_texts[0], gt[\"gold_texts\"]).score\n\n    # 6. Hop Pred (Menggunakan Top-2 Score, tidak terpengaruh K)\n    hop_pred = dynamic_hop_predictor(top_scores, model_name)\n    \n    return {\n        \"model\": model_name, \"k\": k_val, \"question_id\": qid,\n        \"hop_pred\": hop_pred,\n        \"precision\": precision, \"recall\": recall, \n        \"mrr\": mrr, \"ndcg\": ndcg, \"bleu\": best_bleu\n    }\n\n# =============================================================================\n# BAGIAN 5: DEFINISI MODEL\n# =============================================================================\n\n# 1. BM25\nprint(\"Building BM25...\")\ntokenized_corpus = [doc.split() for doc in corpus_texts]\nbm25_model = BM25Okapi(tokenized_corpus)\ndef run_bm25(q, k=MAX_K_EVAL):\n    scores = bm25_model.get_scores(q.split())\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 2. TF-IDF\nprint(\"Building TF-IDF...\")\ntfidf_model = TfidfVectorizer(ngram_range=(1,2), min_df=2)\ntfidf_matrix = tfidf_model.fit_transform(corpus_texts)\ndef run_tfidf(q, k=MAX_K_EVAL):\n    q_vec = tfidf_model.transform([q])\n    scores = (tfidf_matrix @ q_vec.T).toarray().flatten()\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 3. DENSE E5-BASE\nprint(\"Encoding Dense E5-Base...\")\ndense_model = SentenceTransformer('intfloat/multilingual-e5-base', device=device)\ndense_emb = dense_model.encode([\"passage: \" + t for t in corpus_texts], batch_size=64, show_progress_bar=True, normalize_embeddings=True)\ndef run_dense(q, k=MAX_K_EVAL):\n    q_emb = dense_model.encode([\"query: \" + q], normalize_embeddings=True)[0]\n    scores = dense_emb @ q_emb\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 4. HYBRID RRF\ndef run_hybrid(q, k=MAX_K_EVAL):\n    idx_b, _ = run_bm25(q, k=100)\n    idx_d, _ = run_dense(q, k=100)\n    score_map = {}\n    for r, i in enumerate(idx_b): score_map[i] = score_map.get(i, 0) + 1/(60+r)\n    for r, i in enumerate(idx_d): score_map[i] = score_map.get(i, 0) + 1/(60+r)\n    srt = sorted(score_map.items(), key=lambda x: x[1], reverse=True)[:k]\n    return np.array([x[0] for x in srt]), np.array([x[1] for x in srt])\n\n# 5. RERANKER\nprint(\"Loading Reranker...\")\nreranker = CrossEncoder('BAAI/bge-reranker-base', device=device)\ndef run_reranker(q, k=MAX_K_EVAL):\n    cand_idx, _ = run_bm25(q, k=TOP_K_RERANK_CANDIDATES) \n    pairs = [[q, corpus_texts[i]] for i in cand_idx]\n    scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n    srt_loc = np.argsort(scores)[::-1][:k]\n    return cand_idx[srt_loc], scores[srt_loc]\n\nMODELS = {\n    \"BM25\": run_bm25,\n    \"TFIDF_WORD\": run_tfidf,\n    \"DENSE_E5\": run_dense,\n    \"HYBRID_RRF\": run_hybrid,\n    \"RERANKER_BGE\": run_reranker\n}\n\n# =============================================================================\n# BAGIAN 6: EKSEKUSI UTAMA (SEQUENTIAL K EVALUATION)\n# =============================================================================\nresults_storage = {5: [], 15: [], 30: []}\n\nprint(\"\\n>>> MEMULAI EVALUASI SISTEM <<<\")\nprint(\"Strategi: Fetch 100 -> Dedup -> Cut to 30 -> Eval @5, @15, @30\")\n\nfor name, func in MODELS.items():\n    print(f\"\\nProcessing Model: {name} ...\")\n    \n    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df)):\n        qid = row[\"question_id\"]\n        qtext = row[\"question_text_norm\"]\n        \n        if qid not in GLOBAL_QRELS_ENHANCED: continue\n        \n        # 1. RETRIEVAL (Ambil banyak dulu)\n        raw_idx, raw_scores = func(qtext, k=TOP_K_FETCH)\n        \n        # 2. DEDUPLIKASI (Ambil 30 terbaik yang unik)\n        clean_fps, clean_pasals, clean_texts, clean_scores = dedup_results(\n            raw_idx, raw_scores, corpus_docfp, corpus_pasal, corpus_texts, k=MAX_K_EVAL\n        )\n        \n        # 3. HITUNG METRICS UNTUK SETIAP K\n        for k_val in K_LEVELS:\n            res = calculate_metrics_for_k(\n                clean_fps, clean_pasals, clean_texts, clean_scores, \n                qid, name, k_val\n            )\n            if res: results_storage[k_val].append(res)\n\n# =============================================================================\n# BAGIAN 7: LAPORAN (REPORTING)\n# =============================================================================\n\ndef print_k_report(k_val):\n    df = pd.DataFrame(results_storage[k_val])\n    summary = df.groupby(\"model\")[[\"precision\", \"recall\", \"mrr\", \"ndcg\", \"bleu\"]].mean()\n    # Sort by Recall for Consistency\n    summary = summary.sort_values(\"recall\", ascending=False)\n    \n    print(f\"\\n\" + \"=\"*50)\n    print(f\" HASIL EVALUASI PADA K = {k_val}\")\n    print(\"=\"*50)\n    print(summary)\n    \n    print(f\"\\n[Distribus Hop @ K={k_val}]\")\n    print(df.groupby([\"model\", \"hop_pred\"]).size().unstack(fill_value=0))\n    print(\"-\" * 50)\n    \n    # Save per K\n    df.to_csv(f\"{OUT_DIR}/results_k{k_val}.csv\", index=False)\n\n# Cetak Laporan Berurutan\nprint_k_report(5)\nprint_k_report(15)\nprint_k_report(30)\n\n# Gabung semua untuk arsip\nall_res = pd.concat([pd.DataFrame(v) for v in results_storage.values()])\nall_res.to_csv(f\"{OUT_DIR}/all_results_combined.csv\", index=False)\nprint(f\"\\nSelesai! Semua file output tersimpan di folder: {OUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T01:03:39.945808Z","iopub.execute_input":"2025-12-16T01:03:39.946487Z","iopub.status.idle":"2025-12-16T01:17:08.654378Z","shell.execute_reply.started":"2025-12-16T01:03:39.946460Z","shell.execute_reply":"2025-12-16T01:17:08.653702Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-12-16 01:05:11.119460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765847111.326350      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765847111.390616      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Device yang digunakan: cuda\nLoading Data...\nData Loaded. Corpus Size: 40724\nGround Truth Enhanced Built.\nBuilding BM25...\nBuilding TF-IDF...\nEncoding Dense E5-Base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93971afe17da4c41ad812496d9bdd966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56f7f94e60841f09998f3c30e229336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da87dceeda664a6cbee9742f67e818cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf37acf8de44663b90207d84d158303"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e606a259059c4bb2a7789a35890510f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3abd39e21cf4ceeba204b3040cad301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c88f8438c14f459c2f2c8cc31e405c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c7b2554bdb49d39c560212e396c924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bab8b419d2f43fbafcdf60ad4794568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca9f918818274f3683d4c80de6d3047f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/637 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e30adc95fe483a9dc7e828387b68e2"}},"metadata":{}},{"name":"stdout","text":"Loading Reranker...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935050e6d5274d83abacfb5548ef6ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d1b6c6473bb4238874e24d9b66b53e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b35c3e57eb4767988658c8999db1c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357256bfd8c7436797b4004d5fdae9b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d989b884954881b3130b49708baa58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d772a1e3d70146b983ec07cd2b8d6111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"340dc33e04bf43dc81ca5bf6ae6b6213"}},"metadata":{}},{"name":"stdout","text":"\n>>> MEMULAI EVALUASI SISTEM <<<\nStrategi: Fetch 100 -> Dedup -> Cut to 30 -> Eval @5, @15, @30\n\nProcessing Model: BM25 ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e171897800fc4974a2ab1bbd6ee872ed"}},"metadata":{}},{"name":"stdout","text":"\nProcessing Model: TFIDF_WORD ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a869228b7f4059b6773f4a0cab126a"}},"metadata":{}},{"name":"stdout","text":"\nProcessing Model: DENSE_E5 ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ce3c601c1d4e38b4a158bdab53eca0"}},"metadata":{}},{"name":"stdout","text":"\nProcessing Model: HYBRID_RRF ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d0de7784444efdaaecc77a26814b89"}},"metadata":{}},{"name":"stdout","text":"\nProcessing Model: RERANKER_BGE ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74728afbbc0e4388add194b1d58b9cfb"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\n HASIL EVALUASI PADA K = 5\n==================================================\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.125333  0.520000  0.438444  0.427665  31.238513\nRERANKER_BGE   0.125333  0.520000  0.464889  0.463212  34.543020\nBM25           0.114667  0.473333  0.374667  0.375389  23.425236\nTFIDF_WORD     0.114667  0.473333  0.379333  0.386721  22.101976\nDENSE_E5       0.106667  0.453333  0.380889  0.374447  34.835756\n\n[Distribus Hop @ K=5]\nhop_pred      multi  single\nmodel                      \nBM25             35      40\nDENSE_E5         26      49\nHYBRID_RRF        9      66\nRERANKER_BGE     45      30\nTFIDF_WORD       27      48\n--------------------------------------------------\n\n==================================================\n HASIL EVALUASI PADA K = 15\n==================================================\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.048889  0.606667  0.448683  0.455489  31.238513\nTFIDF_WORD     0.049778  0.606667  0.394698  0.430074  22.101976\nRERANKER_BGE   0.048889  0.593333  0.475810  0.487914  34.543020\nBM25           0.047111  0.580000  0.387994  0.410678  23.425236\nDENSE_E5       0.045333  0.573333  0.396312  0.411245  34.835756\n\n[Distribus Hop @ K=15]\nhop_pred      multi  single\nmodel                      \nBM25             35      40\nDENSE_E5         26      49\nHYBRID_RRF        9      66\nRERANKER_BGE     45      30\nTFIDF_WORD       27      48\n--------------------------------------------------\n\n==================================================\n HASIL EVALUASI PADA K = 30\n==================================================\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.027556  0.680000  0.451887  0.472772  31.238513\nTFIDF_WORD     0.027556  0.680000  0.398348  0.446602  22.101976\nBM25           0.026222  0.646667  0.391543  0.426848  23.425236\nDENSE_E5       0.025778  0.640000  0.399863  0.427255  34.835756\nRERANKER_BGE   0.025778  0.633333  0.477924  0.497169  34.543020\n\n[Distribus Hop @ K=30]\nhop_pred      multi  single\nmodel                      \nBM25             35      40\nDENSE_E5         26      49\nHYBRID_RRF        9      66\nRERANKER_BGE     45      30\nTFIDF_WORD       27      48\n--------------------------------------------------\n\nSelesai! Semua file output tersimpan di folder: ir_final_project\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Setelah Revisi Groundtruth","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# HIGH-RECALL & RELIABLE RETRIEVAL SYSTEM (FINAL INTEGRATED CODE)\n# =============================================================================\n\n# 1. INSTALL & IMPORTS\nimport os\nimport re\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# Install libraries if not present\ntry:\n    import rank_bm25\n    import sentence_transformers\n    import sacrebleu\nexcept ImportError:\n    os.system('pip install -q pandas numpy scikit-learn rank-bm25 sentence-transformers sacrebleu tqdm')\n    import rank_bm25\n    import sentence_transformers\n    import sacrebleu\n\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\n\n# Setup Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"[INFO] Device used: {device}\")\n\n# =============================================================================\n# 2. CONFIGURATION\n# =============================================================================\n# Konfigurasi High Recall (Wide Funnel)\nTOP_K_FETCH = 300              # Fetch awal diperlebar ke 300\nTOP_K_RERANK_CANDIDATES = 75   # Reranker menilai 75 kandidat teratas\nMAX_K_EVAL = 30                # Evaluasi sampai rank 30\nK_LEVELS = [5, 15, 30]         # Level pelaporan\nTARGET_ERR_MODEL = \"RERANKER_BGE\" # Model untuk Error Analysis CSV\n\n# PATHS (Sesuaikan jika perlu)\nPASAL_CHUNKS_CSV = \"/kaggle/input/dataset-clean-and-ground-truth/pasal_chunks.csv\"\nQUERIES_CSV      = \"/kaggle/input/dataset-clean-and-ground-truth/ground truth uas  - queries.csv\"\nQRELS_CSV        = \"/kaggle/input/gt-revisi/ground truth uas  - qrels (1).csv\"\n\nOUT_DIR = \"ir_final_project\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# =============================================================================\n# 3. PREPROCESSING & CLEANING UTILS\n# =============================================================================\nprint(\"[INFO] Loading and cleaning data...\")\n\ndef clean_doc_id_aggressive(doc_id):\n    \"\"\"Membersihkan ID dokumen agar variasi penamaan dianggap sama.\"\"\"\n    s = str(doc_id).lower().strip()\n    s = re.sub(r'\\.pdf$', '', s)\n    s = re.sub(r'_\\(code\\)', '', s)\n    s = re.sub(r'[^a-z0-9]', '', s)\n    s = s.replace(\"nomor\", \"no\")\n    return s\n\ndef smart_norm(text: str) -> str:\n    if not isinstance(text, str): text = str(text)\n    text = re.sub(r\"(?<=\\d)O(?=\\d)\", \"0\", text)\n    text = text.lower()\n    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)\n    text = re.sub(r\"[^a-z0-9\\.,\\(\\)\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef canon_pasal(p):\n    p = str(p).strip()\n    p = re.sub(r\"p\\s*a\\s*s\\s*a\\s*l\", \"Pasal\", p, flags=re.I)\n    p = re.sub(r\"\\s+\", \" \", p).strip()\n    m = re.match(r\"(?i)pasal\\s+(\\d+)([a-z]?)\", p)\n    if not m: return p\n    return f\"Pasal {m.group(1)}{m.group(2).upper()}\"\n\n# Load DataFrames\ncorpus_df  = pd.read_csv(PASAL_CHUNKS_CSV)\nqueries_df = pd.read_csv(QUERIES_CSV)\nqrels_df   = pd.read_csv(QRELS_CSV)\n\n# Apply Normalization\ncorpus_df[\"pasal_canon\"] = corpus_df[\"pasal\"].apply(canon_pasal)\ncorpus_df[\"doc_fp\"] = corpus_df[\"doc_id\"].apply(clean_doc_id_aggressive)\ncorpus_df[\"chunk_text_norm\"] = corpus_df[\"chunk_text\"].apply(smart_norm)\n\nqueries_df[\"question_text_norm\"] = queries_df[\"question_text\"].apply(smart_norm)\n\nqrels_df[\"pasal_canon\"] = qrels_df[\"pasal\"].apply(canon_pasal)\nqrels_df[\"doc_fp\"] = qrels_df[\"doc_id\"].apply(clean_doc_id_aggressive)\nqrels_df[\"gold_passage_norm\"] = qrels_df[\"gold_passage\"].astype(str).apply(smart_norm)\n\n# Cache Lists for Speed\ncorpus_texts = corpus_df[\"chunk_text_norm\"].tolist()\ncorpus_docfp = corpus_df[\"doc_fp\"].tolist()\ncorpus_pasal = corpus_df[\"pasal_canon\"].tolist()\n\nprint(f\"[INFO] Corpus Size: {len(corpus_texts)}\")\n\n# =============================================================================\n# 4. GROUND TRUTH BUILDER\n# =============================================================================\n# Mapping text lookup for verification\ndoc_id_to_text = {}\nfor _, row in corpus_df.iterrows():\n    key = (row[\"doc_fp\"], row[\"pasal_canon\"])\n    doc_id_to_text[key] = row[\"chunk_text_norm\"]\n\nGLOBAL_QRELS_ENHANCED = {}\nqrels_sorted = qrels_df.sort_values(by=[\"question_id\", \"pasal_canon\"])\n\nfor qid, sub in qrels_sorted.groupby(\"question_id\"):\n    valid_pasals = set()\n    valid_clean_fps = set()\n    texts_by_pasal = {}\n    gold_texts_list = []\n    \n    for _, r in sub.iterrows():\n        p_name = r[\"pasal_canon\"]\n        d_id = r[\"doc_fp\"]\n        \n        valid_clean_fps.add(d_id)\n        \n        # Collect gold texts for BLEU\n        if isinstance(r[\"gold_passage_norm\"], str) and len(r[\"gold_passage_norm\"]) > 5:\n            gold_texts_list.append(r[\"gold_passage_norm\"])\n        \n        if isinstance(p_name, str):\n            valid_pasals.add(p_name)\n            text_ref = doc_id_to_text.get((d_id, p_name))\n            if text_ref:\n                if p_name not in texts_by_pasal: texts_by_pasal[p_name] = []\n                texts_by_pasal[p_name].append(text_ref)\n            \n    GLOBAL_QRELS_ENHANCED[qid] = {\n        \"valid_pasals\": valid_pasals,\n        \"valid_clean_fps\": valid_clean_fps,\n        \"texts_by_pasal\": texts_by_pasal,\n        \"gold_texts\": list(set(gold_texts_list))\n    }\n\nprint(\"[INFO] Ground Truth Built Successfully.\")\n\n# =============================================================================\n# 5. METRICS & LOGIC (STRICT NDCG + OVERLAP MATCHING)\n# =============================================================================\n\ndef get_overlap_coefficient(text1, text2):\n    \"\"\"Calculates overlap coefficient to handle chunking issues.\"\"\"\n    tokens_a = set(str(text1).lower().split())\n    tokens_b = set(str(text2).lower().split())\n    if len(tokens_a) == 0 or len(tokens_b) == 0: return 0.0\n    \n    intersection = len(tokens_a.intersection(tokens_b))\n    min_len = min(len(tokens_a), len(tokens_b))\n    return intersection / min_len if min_len > 0 else 0\n\ndef extract_best_segment_bleu(full_text, ref_texts):\n    \"\"\"Finds best segment match for BLEU scoring.\"\"\"\n    if not isinstance(full_text, str) or not ref_texts: return 0.0\n    text_clean = full_text.replace(\" ayat \", \". ayat \").replace(\" pasal \", \". pasal \")\n    segments = re.split(r'[\\.\\?!]\\s+', text_clean)\n    \n    best_segment_score = 0.0\n    for seg in segments:\n        seg = seg.strip()\n        if len(seg) < 10: continue\n        try:\n            score = sacrebleu.sentence_bleu(seg, ref_texts).score\n            if score > best_segment_score: best_segment_score = score\n        except: pass\n    return best_segment_score\n\ndef dynamic_hop_predictor(scores, model_type):\n    if len(scores) < 2: return \"single\"\n    s1, s2 = scores[0], scores[1]\n    if \"RERANKER\" in model_type:\n        s1 = 1/(1+np.exp(-s1)); s2 = 1/(1+np.exp(-s2))\n    \n    if s1 <= 1e-9: return \"single\"\n    ratio = s2 / s1\n    return \"multi\" if ratio > 0.95 else \"single\"\n\ndef is_prediction_correct(pred_fp, pred_pasal, pred_text, gt_data):\n    # 1. Check ID Match (Clean Aggressive)\n    if pred_fp in gt_data[\"valid_clean_fps\"]: return True\n    \n    # 2. Check Content Overlap (Threshold 0.5 for reliability)\n    if pred_pasal in gt_data[\"valid_pasals\"]:\n        valid_texts = gt_data[\"texts_by_pasal\"].get(pred_pasal, [])\n        for val_text in valid_texts:\n            if get_overlap_coefficient(pred_text, val_text) >= 0.5: \n                return True\n    return False\n\ndef calculate_metrics_strict(pred_fps, pred_pasals, pred_texts, top_scores, qid, model_name, k_val):\n    if qid not in GLOBAL_QRELS_ENHANCED: return None\n    gt = GLOBAL_QRELS_ENHANCED[qid]\n    \n    curr_fps = pred_fps[:k_val]\n    curr_pasals = pred_pasals[:k_val]\n    curr_texts = pred_texts[:k_val]\n    \n    hits_raw = []\n    hits_unique_ndcg = []\n    seen_pasals_ndcg = set()\n    found_unique_pasals = set()\n    \n    for i in range(len(curr_fps)):\n        is_hit = is_prediction_correct(curr_fps[i], curr_pasals[i], curr_texts[i], gt)\n        \n        if is_hit:\n            hits_raw.append(1)\n            \n            # Identify logic for Recall & NDCG Deduplication\n            detected_pasal = curr_pasals[i]\n            if detected_pasal not in gt[\"valid_pasals\"]:\n                detected_pasal = f\"detected_content_match_{i}\"\n\n            if curr_pasals[i] in gt[\"valid_pasals\"]:\n                found_unique_pasals.add(curr_pasals[i])\n            else:\n                found_unique_pasals.add(detected_pasal)\n\n            # Strict NDCG: Only count first occurrence of a relevant pasal\n            if detected_pasal in seen_pasals_ndcg:\n                hits_unique_ndcg.append(0)\n            else:\n                hits_unique_ndcg.append(1)\n                seen_pasals_ndcg.add(detected_pasal)\n        else:\n            hits_raw.append(0)\n            hits_unique_ndcg.append(0)\n            \n    # --- RECALL ---\n    true_matches = found_unique_pasals.intersection(gt[\"valid_pasals\"])\n    tp_unique = len(true_matches)\n    if tp_unique == 0 and sum(hits_raw) > 0: tp_unique = 1 # Minimum 1 if content match exists\n    \n    total_gt = len(gt[\"valid_pasals\"])\n    recall = tp_unique / total_gt if total_gt > 0 else 0\n    recall = min(recall, 1.0)\n    \n    # --- PRECISION ---\n    precision = sum(hits_raw) / k_val\n    \n    # --- NDCG (Strict) ---\n    def dcg(r): return sum((2**v - 1)/math.log2(idx+2) for idx,v in enumerate(r))\n    ideal = [1] * total_gt\n    actual_dcg = dcg(hits_unique_ndcg)\n    ideal_dcg = dcg(ideal)\n    ndcg = actual_dcg / (ideal_dcg + 1e-9)\n    ndcg = min(ndcg, 1.0)\n    \n    # --- MRR ---\n    mrr = 0\n    for i, h in enumerate(hits_raw):\n        if h: mrr = 1/(i+1); break\n            \n    # --- BLEU ---\n    best_bleu = 0.0\n    if gt[\"gold_texts\"] and len(curr_texts) > 0:\n        best_bleu = extract_best_segment_bleu(curr_texts[0], gt[\"gold_texts\"])\n\n    hop_pred = dynamic_hop_predictor(top_scores, model_name)\n    \n    return {\n        \"model\": model_name, \"k\": k_val, \"question_id\": qid, \"hop_pred\": hop_pred,\n        \"precision\": precision, \"recall\": recall, \"mrr\": mrr, \"ndcg\": ndcg, \"bleu\": best_bleu\n    }\n\ndef dedup_results(idx, scores, corpus_docfp, corpus_pasal, corpus_texts, k=30):\n    seen_fp = set(); u_fps, u_pasals, u_texts, u_scores = [], [], [], []\n    for i, s in zip(idx, scores):\n        fp = str(corpus_docfp[i])\n        if fp in seen_fp: continue\n        seen_fp.add(fp)\n        u_fps.append(fp); u_pasals.append(corpus_pasal[i]); u_texts.append(corpus_texts[i]); u_scores.append(s)\n        if len(u_fps) >= k: break\n    return u_fps, u_pasals, u_texts, np.array(u_scores)\n\n# =============================================================================\n# 6. MODEL INITIALIZATION\n# =============================================================================\nprint(\"[INFO] Initializing Retrieval Models...\")\n\n# 1. BM25\ntokenized_corpus = [doc.split() for doc in corpus_texts]\nbm25_model = BM25Okapi(tokenized_corpus)\ndef run_bm25(q, k=MAX_K_EVAL):\n    scores = bm25_model.get_scores(q.split())\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 2. TF-IDF\ntfidf_model = TfidfVectorizer(ngram_range=(1,2), min_df=2)\ntfidf_matrix = tfidf_model.fit_transform(corpus_texts)\ndef run_tfidf(q, k=MAX_K_EVAL):\n    q_vec = tfidf_model.transform([q])\n    scores = (tfidf_matrix @ q_vec.T).toarray().flatten()\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 3. DENSE E5\ndense_model = SentenceTransformer('intfloat/multilingual-e5-base', device=device)\ndense_emb = dense_model.encode([\"passage: \" + t for t in corpus_texts], batch_size=64, show_progress_bar=False, normalize_embeddings=True)\ndef run_dense(q, k=MAX_K_EVAL):\n    q_emb = dense_model.encode([\"query: \" + q], normalize_embeddings=True)[0]\n    scores = dense_emb @ q_emb\n    top_n = np.argsort(scores)[::-1][:k]\n    return top_n, scores[top_n]\n\n# 4. HYBRID RRF\ndef run_hybrid(q, k=MAX_K_EVAL):\n    idx_b, _ = run_bm25(q, k=TOP_K_FETCH) \n    idx_d, _ = run_dense(q, k=TOP_K_FETCH)\n    score_map = {}\n    for r, i in enumerate(idx_b): score_map[i] = score_map.get(i, 0) + 1/(60+r)\n    for r, i in enumerate(idx_d): score_map[i] = score_map.get(i, 0) + 1/(60+r)\n    srt = sorted(score_map.items(), key=lambda x: x[1], reverse=True)[:k]\n    return np.array([x[0] for x in srt]), np.array([x[1] for x in srt])\n\n# 5. RERANKER\nreranker = CrossEncoder('BAAI/bge-reranker-base', device=device)\ndef run_reranker(q, k=MAX_K_EVAL):\n    cand_idx, _ = run_hybrid(q, k=TOP_K_RERANK_CANDIDATES) \n    pairs = [[q, corpus_texts[i]] for i in cand_idx]\n    scores = reranker.predict(pairs, batch_size=32, show_progress_bar=False)\n    srt_loc = np.argsort(scores)[::-1][:k]\n    return cand_idx[srt_loc], scores[srt_loc]\n\nMODELS = {\n    \"BM25\": run_bm25,\n    \"TFIDF_WORD\": run_tfidf,\n    \"DENSE_E5\": run_dense,\n    \"HYBRID_RRF\": run_hybrid,\n    \"RERANKER_BGE\": run_reranker\n}\n\n# =============================================================================\n# 7. EXECUTE EVALUATION LOOP\n# =============================================================================\nresults_storage = {5: [], 15: [], 30: []}\nprint(\"[INFO] Starting Evaluation...\")\n\nfor name, func in MODELS.items():\n    print(f\"[PROCESSING] Model: {name}\")\n    for _, row in tqdm(queries_df.iterrows(), total=len(queries_df), leave=False):\n        qid = row[\"question_id\"]\n        if qid not in GLOBAL_QRELS_ENHANCED: continue\n        \n        # Retrieval (Wide Fetch)\n        raw_idx, raw_scores = func(row[\"question_text_norm\"], k=TOP_K_FETCH)\n        \n        # Deduplication & Cutoff\n        clean_fps, clean_pasals, clean_texts, clean_scores = dedup_results(\n            raw_idx, raw_scores, corpus_docfp, corpus_pasal, corpus_texts, k=MAX_K_EVAL\n        )\n        \n        # Calculate Metrics\n        for k_val in K_LEVELS:\n            res = calculate_metrics_strict(\n                clean_fps, clean_pasals, clean_texts, clean_scores, \n                qid, name, k_val\n            )\n            if res: results_storage[k_val].append(res)\n\n# =============================================================================\n# 8. GENERATE REPORTS & ERROR ANALYSIS\n# =============================================================================\n\n\nprint(f\"[INFO] Process Complete. Results saved to {OUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:04:53.111069Z","iopub.execute_input":"2025-12-16T10:04:53.111346Z","iopub.status.idle":"2025-12-16T10:21:06.528836Z","shell.execute_reply.started":"2025-12-16T10:04:53.111328Z","shell.execute_reply":"2025-12-16T10:21:06.527972Z"}},"outputs":[{"name":"stdout","text":"[INFO] Device used: cuda\n[INFO] Loading and cleaning data...\n[INFO] Corpus Size: 40724\n[INFO] Ground Truth Built Successfully.\n[INFO] Initializing Retrieval Models...\n[INFO] Starting Evaluation...\n[PROCESSING] Model: BM25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[PROCESSING] Model: TFIDF_WORD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[PROCESSING] Model: DENSE_E5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[PROCESSING] Model: HYBRID_RRF\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[PROCESSING] Model: RERANKER_BGE\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n[METRICS SUMMARY K=5]\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.448000  0.477164  0.689778  0.531610  29.649171\nBM25           0.421333  0.429386  0.668889  0.496616  28.929381\nRERANKER_BGE   0.373333  0.407037  0.602222  0.455364  28.099498\nDENSE_E5       0.360000  0.392831  0.593778  0.451754  25.333727\nTFIDF_WORD     0.357333  0.381275  0.576000  0.445950  19.890320\n\n[METRICS SUMMARY K=15]\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.310222  0.533720  0.696296  0.565677  29.649171\nBM25           0.296889  0.522180  0.678063  0.544833  28.929381\nRERANKER_BGE   0.285333  0.504307  0.611735  0.514190  28.099498\nDENSE_E5       0.252444  0.484386  0.608289  0.503028  25.333727\nTFIDF_WORD     0.250667  0.454942  0.583005  0.492984  19.890320\n\n[METRICS SUMMARY K=30]\n              precision    recall       mrr      ndcg       bleu\nmodel                                                           \nHYBRID_RRF     0.232889  0.578418  0.698271  0.583030  29.649171\nBM25           0.213333  0.567011  0.679888  0.560757  28.929381\nRERANKER_BGE   0.236889  0.553196  0.613754  0.528501  28.099498\nDENSE_E5       0.203111  0.527720  0.610886  0.525071  25.333727\nTFIDF_WORD     0.188889  0.503831  0.586172  0.514110  19.890320\n\n[INFO] Generating Error Analysis for RERANKER_BGE...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[INFO] Process Complete. Results saved to ir_final_project\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"TARGET_ERR_MODEL = \"HYBRID_RRF\"\n\n# Error Analysis (CSV Output)\nprint(f\"\\n[INFO] Generating Error Analysis for {TARGET_ERR_MODEL}...\")\nerr_rows = []\nretrieve_func = MODELS[TARGET_ERR_MODEL]\n\nfor _, row in tqdm(queries_df.iterrows(), total=len(queries_df), leave=False):\n    qid = row[\"question_id\"]\n    if qid not in GLOBAL_QRELS_ENHANCED: continue\n    gt = GLOBAL_QRELS_ENHANCED[qid]\n    \n    # Get Top-1\n    raw_idx, raw_scores = retrieve_func(row[\"question_text_norm\"], k=1)\n    \n    pred_doc = \"-\"\n    pred_pasal = \"-\"\n    pred_text = \"-\"\n    status = \"MISS\"\n    score = 0.0\n    \n    if len(raw_idx) > 0:\n        idx = raw_idx[0]\n        pred_doc = str(corpus_docfp[idx])\n        pred_pasal = corpus_pasal[idx]\n        pred_text = corpus_texts[idx]\n        \n        is_hit = is_prediction_correct(pred_doc, pred_pasal, pred_text, gt)\n        \n        # Determine status details\n        if is_hit:\n            if pred_doc in gt[\"valid_clean_fps\"]:\n                status = \"CORRECT (ID MATCH)\"\n            else:\n                status = \"CORRECT (CONTENT MATCH)\"\n        else:\n            # Check overlap for context\n            best_ov = 0\n            if gt[\"texts_by_pasal\"]:\n                for p_key in gt[\"texts_by_pasal\"]:\n                    for t in gt[\"texts_by_pasal\"][p_key]:\n                        ov = get_overlap_coefficient(pred_text, t)\n                        if ov > best_ov: best_ov = ov\n            \n            if best_ov > 0.3: status = \"WRONG (CLOSE)\"\n            else: status = \"WRONG (IRRELEVANT)\"\n\n    err_rows.append({\n        \"Query\": row[\"question_text\"],\n        \"GT Pasal\": \", \".join(gt[\"valid_pasals\"]),\n        \"Pred Pasal\": pred_pasal,\n        \"Pred Doc\": pred_doc,\n        \"Status\": status,\n        \"Pred Text\": pred_text[:200]\n    })\n\ndf_err = pd.DataFrame(err_rows)\ndf_err.to_csv(f\"{OUT_DIR}/error_analysis_{TARGET_ERR_MODEL}.csv\", index=False)\nprint(f\"[INFO] Process Complete. Results saved to {OUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T10:23:13.651989Z","iopub.execute_input":"2025-12-16T10:23:13.652286Z","iopub.status.idle":"2025-12-16T10:23:30.013236Z","shell.execute_reply.started":"2025-12-16T10:23:13.652264Z","shell.execute_reply":"2025-12-16T10:23:30.012384Z"}},"outputs":[{"name":"stdout","text":"\n[INFO] Generating Error Analysis for HYBRID_RRF...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[INFO] Process Complete. Results saved to ir_final_project\n","output_type":"stream"}],"execution_count":13}]}